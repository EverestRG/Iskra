import os
from flask import Flask, request, jsonify
from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoModelForCausalLM, AutoTokenizer
from emotion_index import calculate_happiness_index
import torch, re, random

print("Loading model...")

app = Flask(__name__)

# –ü—É—Ç—å –∫ –º–æ–¥–µ–ª—è–º
gpt2_model_path = "./models/gpt2"
dialo_model_path = "./models/dialo"
dialol_model_path = "./models/dialolarge"
savepth = "./save.txt"
dialog_history = []
dialog_history_tokens = []

# –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏: gpt2 –∏–ª–∏ dialo
model_choice = "dialolarge"  # –ú–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å –Ω–∞ "gpt2" –∏–ª–∏ "dialo"

# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –ø–∞–ø–∫–∞ —Å –º–æ–¥–µ–ª—å—é
def load_model(model_choice):
    if model_choice == "gpt2":
        model_path = gpt2_model_path
        model_class = GPT2LMHeadModel
        tokenizer_class = GPT2Tokenizer
    elif model_choice == "dialo":
        model_path = dialo_model_path
        model_class = AutoModelForCausalLM
        tokenizer_class = AutoTokenizer
    elif model_choice == "dialolarge":
        model_path = dialol_model_path
        model_class = AutoModelForCausalLM
        tokenizer_class = AutoTokenizer
    else:
        raise ValueError("Unsupported model choice")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –ø–∞–ø–∫–∞ —Å –º–æ–¥–µ–ª—å—é
    if not os.path.exists(model_path):
        print(f"Model folder '{model_path}' not found, downloading...")
        if model_choice == "dialo":
            model_name = "microsoft/DialoGPT-medium"  # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 'microsoft/DialoGPT-large' –∏–ª–∏ 'microsoft/DialoGPT-small'
        elif model_choice == "gpt2":
            model_name = "gpt2-medium"
        elif model_choice == "dialolarge":
            model_name = "microsoft/DialoGPT-large"
        model = model_class.from_pretrained(model_name)
        tokenizer = tokenizer_class.from_pretrained(model_name)
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤ –ø–∞–ø–∫—É
        os.makedirs(model_path, exist_ok=True)
        model.save_pretrained(model_path)
        tokenizer.save_pretrained(model_path)
    else:
        print(f"Loading model from {model_path}...")
        model = model_class.from_pretrained(model_path)
        tokenizer = tokenizer_class.from_pretrained(model_path)

    return model, tokenizer


# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
model, tokenizer = load_model(model_choice)

print("Model loaded!")


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
def clean_generated_text(text):
    """
    –£–±–∏—Ä–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –±—É–∫–≤—ã –∏ —É–¥–∞–ª—è–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞,
    –∫—Ä–æ–º–µ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π.
    """
    # –ò—Å–∫–ª—é—á–µ–Ω–∏—è –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤
    short_word_exceptions = {"i", "im", "ur", "am", "on", "of", "an", "a", "it", "is", "in", "at", "to", "by", "as", "he", "we", "do", "be", "go", "me", "my", "no", "or", "up", "us"}

    text = text.replace(' DDD', '')
    text = text.replace('DDD', '')
    text = text.replace('And thanks again.', '')
    text = text.replace('Thanks again.', '')
    text = text.replace('Thanks again', '')

    # –£–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
    text = re.sub(r'[^\w\s.,!?]', '', text)

    # –£–¥–∞–ª—è–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞, –∫—Ä–æ–º–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
    filtered_words = [
        word for word in text.split()
        if len(word) > 2 or word.lower() in short_word_exceptions
    ]

    # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –æ–±—Ä–∞—Ç–Ω–æ
    text = ' '.join(filtered_words)

    filtered_words = [
        word for word in text.split()
        if len(word.replace('.', '').replace('!', '').replace('?', '')) > 2 or word.lower() in short_word_exceptions
    ]

    # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –æ–±—Ä–∞—Ç–Ω–æ
    text = ' '.join(filtered_words)

    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text).strip()

    return text

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —ç–º–æ—Ü–∏–∏ –≤ –æ—Ç–≤–µ—Ç
def add_emotion(text, emotion=0.0):
    print(emotion)
    if emotion > 0.99979:
        return text + ' ' + random.choice(["üòÉ", "üòÑ", "üòä"])
    if emotion < -0.99979:
        return text + ' ' + random.choice(["üôÅ", "üò•", "üòñ"])
    return text


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–µ–∑–∫–∏ —Ç–µ–∫—Å—Ç–∞ –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–≤–µ—Ä—à—ë–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
def trim_to_sentence(text):
    last_punctuation = max(text.rfind('.'), text.rfind('!'), text.rfind('?'))  # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∑–Ω–∞–∫ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    if last_punctuation != -1:
        return text[:last_punctuation + 1].strip()  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—Å—Ç –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–Ω–∞–∫–∞
    return text.strip()  # –ï—Å–ª–∏ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –Ω–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
def generate_text(prompt):
    global dialog_history_tokens, dialog_history

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–º–æ–Ω–æ–ª–æ–≥ –æ—Ç –ª–∏—Ü–∞ System)
    static_instructions = (
        f"<|user|> Your task is to respond to my queries clearly with kindness.\n<|iskra|> Understood.{tokenizer.eos_token}"
        #f"\nCurrently, you feel {current_emotion}."
    )

    # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é —á–∞—Å—Ç—å –∏—Å—Ç–æ—Ä–∏–∏ (–∏—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞)
    conversation_history = ""
    for i in range(0, len(dialog_history_tokens), 2):  # –ß–µ—Ä–µ–¥—É–µ–º –∑–∞–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã
        user_input = tokenizer.decode(dialog_history_tokens[i][0], skip_special_tokens=True)
        bot_response = tokenizer.decode(dialog_history_tokens[i + 1][0], skip_special_tokens=True) if i + 1 < len(dialog_history_tokens) else ""
        conversation_history += f"\n<|user|> {user_input}\n<|iskra|> {bot_response}{tokenizer.eos_token}"

    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å
    conversation_history += f"\n<|user|> {prompt}"

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
    #context = "".join([static_instructions, conversation_history])

    context = conversation_history

    print(context)

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    inputs = tokenizer.encode(context + tokenizer.eos_token, return_tensors="pt", truncation=True, max_length=1024)
    #inputs = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors="pt", truncation=True, max_length=1024)

    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å –≤ –∏—Å—Ç–æ—Ä–∏—é
    dialog_history_tokens.append(tokenizer.encode(f"{prompt}", return_tensors="pt"))

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∏—Å—Ç–æ—Ä–∏–∏
    if len(dialog_history_tokens) > 2:  # 3 –ø–∞—Ä—ã (–≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç)
        dialog_history_tokens.pop(0)
        dialog_history_tokens.pop(0)

    # –ú–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è
    attention_mask = torch.ones(inputs.shape, device=inputs.device)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
    outputs = model.generate(
        inputs,
        max_length=300,
        num_return_sequences=1,
        no_repeat_ngram_size=2,
        temperature=0.35,
        top_k=48,
        top_p=0.8,
        max_new_tokens=25,
        repetition_penalty=2.0,
        pad_token_id=tokenizer.eos_token_id,
        eos_token_id=tokenizer.eos_token_id,
        attention_mask=attention_mask,
        do_sample=True
    )

    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
    text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∏–∑ –æ—Ç–≤–µ—Ç–∞
    text = text.replace(tokenizer.decode(inputs[0], skip_special_tokens=True), '')

    # –ß–∏—Å—Ç–∏–º —Ç–µ–∫—Å—Ç
    text = clean_generated_text(text)
    text = trim_to_sentence(text)

    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏—é
    response_tokens = tokenizer.encode(f"{text}", return_tensors="pt")
    dialog_history_tokens.append(response_tokens)

    # –î–æ–±–∞–≤–ª—è–µ–º —ç–º–æ—Ü–∏—é
    text = add_emotion(text, calculate_happiness_index(text))

    print(f"Generated text: {text}")

    dialog_history.append(f"{prompt} -> {text}")

    return text

@app.route('/get_response', methods=['POST'])
def get_response():
    data = request.get_json()
    prompt = data.get('prompt', '')
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    if prompt:
        response_text = generate_text(prompt)
        return jsonify({"response": response_text})
    else:
        return jsonify({"response": "No prompt provided"}), 400


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
    print('Saving dialog context')
    with open(savepth, 'w+', encoding="utf-8") as save:
        save.write('\n'.join(dialog_history))