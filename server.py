import os
from flask import Flask, request, jsonify
from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoModelForCausalLM, AutoTokenizer
#from emotion_index import calculate_happiness_index
import torch, re, random
from contextshift import detect_context_shift
from  responsesranker import rank_responses

print("Loading model...")

app = Flask(__name__)

# –ü—É—Ç—å –∫ –º–æ–¥–µ–ª—è–º
gpt2_model_path = "./models/gpt2"
dialo_model_path = "./models/dialo"
dialol_model_path = "./models/dialolarge"
rugpt_model_path = "./models/rugpt"

# –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏: gpt2 –∏–ª–∏ dialo
model_choice = "rugpt"  # –ú–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å –Ω–∞ "gpt2" –∏–ª–∏ "dialo"

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∏ –∏—Å—Ç–æ—Ä–∏–∏
def load_user_history(user_id):
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
    history_file = os.path.join('history', f'{user_id}.txt')
    token_history_file = os.path.join('token_history', f'{user_id}.txt')

    if os.path.exists(history_file):
        with open(history_file, 'r', encoding='utf-8') as f:
            user_history = f.read().split('%NEXT%')
    else:
        user_history = []

    user_token_history = []

    if os.path.exists(token_history_file):
        with open(token_history_file, 'r', encoding='utf-8') as f:
            readed = f.readlines()
            for i in range(len(readed)):
                user_token_history.append(tokenizer.encode(readed[i], return_tensors="pt", truncation=True, max_length=1024))

    return user_history, user_token_history

def save_user_history(user_id, user_history, user_token_history):
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
    os.makedirs('history', exist_ok=True)
    os.makedirs('token_history', exist_ok=True)

    history_file = os.path.join('history', f'{user_id}.txt')
    token_history_file = os.path.join('token_history', f'{user_id}.txt')

    with open(history_file, 'w', encoding='utf-8') as f:
        f.write('%NEXT%'.join(user_history))

    with open(token_history_file, 'w', encoding='utf-8') as f:
        story = []
        for i in range(len(user_token_history)):
            story.append(tokenizer.decode(user_token_history[i][0], skip_special_tokens=True).replace('\n', ''))
        while True:
            try:
                story.pop(story.index(''))
            except:
                break
        f.write('\n'.join(story))

# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –ø–∞–ø–∫–∞ —Å –º–æ–¥–µ–ª—å—é
def load_model(model_choice):
    if model_choice == "gpt2":
        model_path = gpt2_model_path
        model_class = GPT2LMHeadModel
        tokenizer_class = GPT2Tokenizer
    elif model_choice == "dialo":
        model_path = dialo_model_path
        model_class = AutoModelForCausalLM
        tokenizer_class = AutoTokenizer
    elif model_choice == "dialolarge":
        model_path = dialol_model_path
        model_class = AutoModelForCausalLM
        tokenizer_class = AutoTokenizer
    elif model_choice == "rugpt":
        model_path = rugpt_model_path
        model_class = AutoModelForCausalLM
        tokenizer_class = AutoTokenizer
    else:
        raise ValueError("Unsupported model choice")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –ø–∞–ø–∫–∞ —Å –º–æ–¥–µ–ª—å—é
    if not os.path.exists(model_path):
        print(f"Model folder '{model_path}' not found, downloading...")
        if model_choice == "dialo":
            model_name = "microsoft/DialoGPT-medium"  # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 'microsoft/DialoGPT-large' –∏–ª–∏ 'microsoft/DialoGPT-small'
        elif model_choice == "gpt2":
            model_name = "gpt2-medium"
        elif model_choice == "dialolarge":
            model_name = "microsoft/DialoGPT-large"
        elif model_choice == "rugpt":
            model_name = "Kirili4ik/ruDialoGpt3-medium-finetuned-telegram"
        model = model_class.from_pretrained(model_name)
        tokenizer = tokenizer_class.from_pretrained(model_name)
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤ –ø–∞–ø–∫—É
        os.makedirs(model_path, exist_ok=True)
        model.save_pretrained(model_path)
        tokenizer.save_pretrained(model_path)
    else:
        print(f"Loading model from {model_path}...")
        model = model_class.from_pretrained(model_path)
        tokenizer = tokenizer_class.from_pretrained(model_path)

    return model, tokenizer


# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
model, tokenizer = load_model(model_choice)

print("Model loaded!")


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
def clean_generated_text(text):
    """
    –£–±–∏—Ä–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –±—É–∫–≤—ã –∏ —É–¥–∞–ª—è–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞,
    –∫—Ä–æ–º–µ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π.
    """
    # –ò—Å–∫–ª—é—á–µ–Ω–∏—è –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤
    short_word_exceptions = {"-", "—É", "–æ–∫", "—Ö–µ", "—Ö–∏", "–¥–∞", "–∞", "—Ö–∞", "—è", "—Ç—ã", "—Ç–µ", "—Ç—É", "—Ç–æ", "–æ–Ω", "–Ω–µ", "–Ω–æ", "–∏", "–ø–æ", "—Å", "—Å–æ", "–Ω–∞", "–æ—Ç", "–∑–∞", "–≤"}

    filtered_words = [
        word for word in text.split()
        if len(word.replace('.', '').replace('!', '').replace('?', '')) > 2 or word.lower() in short_word_exceptions
    ]

    # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –æ–±—Ä–∞—Ç–Ω–æ
    #text = ' '.join(filtered_words)

    text = text.replace(')))', '')
    text = text.replace(' ) ', '')
    text = text.replace('(((', '')
    text = text.replace(' ( ', '')

    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text).strip()

    return text

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —ç–º–æ—Ü–∏–∏ –≤ –æ—Ç–≤–µ—Ç
def add_emotion(text, emotion=0.0):
    print(emotion)
    if emotion > 0.99979:
        return text + ' ' + random.choice(["üòÉ", "üòÑ", "üòä"])
    if emotion < -0.99979:
        return text + ' ' + random.choice(["üôÅ", "üò•", "üòñ"])
    return text


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–µ–∑–∫–∏ —Ç–µ–∫—Å—Ç–∞ –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–≤–µ—Ä—à—ë–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
def trim_to_sentence(text):
    last_punctuation = max(text.rfind('.'), text.rfind('!'), text.rfind('?'))  # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∑–Ω–∞–∫ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    if last_punctuation != -1:
        return text[:last_punctuation + 1].strip()  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—Å—Ç –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–Ω–∞–∫–∞
    return text.strip()  # –ï—Å–ª–∏ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –Ω–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
def generate_text(prompt, user_history, user_token_history):
    global model, tokenizer

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–æ–º–∞–Ω–¥—É —Å–±—Ä–æ—Å–∞ –∏—Å—Ç–æ—Ä–∏–∏
    if prompt == "%%<|RESET|>%%":
        user_history.clear()  # –û—á–∏—â–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π
        user_token_history.clear()  # –û—á–∏—â–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é —Ç–æ–∫–µ–Ω–æ–≤
        return "%|DONE|%", user_history, user_token_history  # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —Å–±—Ä–æ—Å–∞

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
    static_instructions = "User1: –ü—Ä–∏–≤–µ—Ç, —è –ò—Å–∫—Ä–∞! –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?"

    def txtlen(string: str) -> str:
        length = len(tokenizer.encode(string))
        if length <= 15:
            return '1'
        if length <= 50:
            return '2'
        if length <= 256:
            return '3'
        return '-'

    shifted = False

    if len(user_token_history) > 0:
        outs = []
        for i in range(len(user_token_history)):
            tt = tokenizer.decode(user_token_history[i][0], skip_special_tokens=True).replace('\n', '')
            if tt.endswith(('.', '!', '?')):
                outs.append(tt)
            else:
                outs.append(tt + '.')
        outs.append(prompt)
        shifted = detect_context_shift(outs, 0.42)

    if shifted:
        user_token_history.clear()

    # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é —á–∞—Å—Ç—å –∏—Å—Ç–æ—Ä–∏–∏
    conversation_history = ""
    for i in range(0, len(user_token_history), 2):
        user_input = tokenizer.decode(user_token_history[i][0], skip_special_tokens=True).replace('\n', '')
        bot_response = tokenizer.decode(user_token_history[i + 1][0], skip_special_tokens=True).replace('\n', '') if i + 1 < len(user_token_history) else ""
        conversation_history += f"|0|{txtlen(user_input)}|{user_input}\n|1|{txtlen(bot_response)}||0|1|{bot_response}\n"

    clean_context = conversation_history
    conversation_history += f"|0|{txtlen(prompt)}|{prompt}{tokenizer.eos_token}|1|2|"

    context = conversation_history

    #context = f"{prompt + tokenizer.eos_token}"

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    inputs = tokenizer.encode(context, return_tensors="pt", truncation=True, max_length=1024)

    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å –≤ –∏—Å—Ç–æ—Ä–∏—é
    user_token_history.append(tokenizer.encode(f"{prompt}", return_tensors="pt"))

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∏—Å—Ç–æ—Ä–∏–∏
    if len(user_token_history) > 4:  # 1 –ø–∞—Ä–∞ (–≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç)
        user_token_history.pop(0)
        user_token_history.pop(0)

    attention_mask = torch.ones(inputs.shape, device=inputs.device)
    #attention_mask = (inputs != tokenizer.pad_token_id).long()

    print(context)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
    outputs = model.generate(
        inputs,
        max_length=256,
        early_stopping=True,
        num_return_sequences=2,
        no_repeat_ngram_size=3,
        num_beams=4,
        temperature=0.65,
        top_k=70,
        top_p=0.85,
        max_new_tokens=100,
        repetition_penalty=1.4,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
        attention_mask=attention_mask,
        do_sample=True
    )

    responses = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]
    responsesclean = [response.replace(context.replace(tokenizer.eos_token, '').replace(tokenizer.bos_token, ''), '') for response in responses]

    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
    #if not clean_context == "":
    #    text = rank_responses(prompt, responsesclean, clean_context)
    #else:
    #    text = rank_responses(prompt, responsesclean)

    text = rank_responses(prompt, responsesclean)

    # –ß–∏—Å—Ç–∏–º —Ç–µ–∫—Å—Ç
    text = clean_generated_text(text)
    #text = trim_to_sentence(text)

    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏—é
    response_tokens = tokenizer.encode(f"{text}", return_tensors="pt")
    user_token_history.append(response_tokens)

    # –î–æ–±–∞–≤–ª—è–µ–º —ç–º–æ—Ü–∏—é
    #if random.choice([0, 1, 1, 1]):
    #    text = add_emotion(text, calculate_happiness_index(text))

    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞
    user_history.append(f"[You]: {prompt}\n\n[Iskra]: {text}")

    text = '\n\n'.join(user_history)

    return text, user_history, user_token_history

app.config['SEND_FILE_MAX_AGE_DEFAULT'] = 60

@app.route('/get_response', methods=['POST'])
def get_response():
    data = request.get_json()
    prompt = data.get('prompt', '')
    user_id = data.get('user_id')  # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

    if not user_id:
        return jsonify({"response": "User ID is required"}), 400

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    user_history, user_token_history = load_user_history(user_id)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏—Å—Ç–æ—Ä–∏–∏ —ç—Ç–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    response_text, user_history, user_token_history = generate_text(prompt, user_history, user_token_history)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    save_user_history(user_id, user_history, user_token_history)

    return jsonify({"response": response_text}), 400

@app.route('/get_history', methods=['POST'])
def get_history():
    data = request.get_json()
    user_id = data.get('user_id')  # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

    if not user_id:
        return jsonify({"response": "User ID is required"}), 400

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    user_history, user_token_history = load_user_history(user_id)

    return jsonify({"response": "\n\n".join(user_history)}), 400


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
